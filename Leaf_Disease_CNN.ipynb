{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_ymLuhTqTVci"
   },
   "source": [
    "### One time run for dataset division\n",
    "\n",
    "Here a dataset division in training-validation-test sets is applied. The results are three zip, one for each set.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C-uPEIopguTk"
   },
   "outputs": [],
   "source": [
    "# Mount Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MEgRwxeuXkcU"
   },
   "outputs": [],
   "source": [
    "# Go in the folder where the zip file of the dataset is\n",
    "%cd '/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1viLb7Q0TJxq"
   },
   "outputs": [],
   "source": [
    "# Unzip the dataset (zip file) in a temporary folder on the virtual machine.\n",
    "!unzip ./color.zip -d /content/localdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9PiQn3YUVdK"
   },
   "outputs": [],
   "source": [
    "##### Create Train - Val - Test folders #####\n",
    "# Takes the unzipped files of the dataset from the root_dir (files should be\n",
    "# divided into different subfolders - one for each class) and creates a folder\n",
    "# for each set in the root_root_dir containing the split part of files.\n",
    "# For each class, files are randomly divided.\n",
    "#####\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import random\n",
    "root_root_dir = '/content/localdata/'\n",
    "root_dir = '/content/localdata/color' \n",
    "classes_dir = []\n",
    "for i in os.listdir(root_dir):\n",
    "    classes_dir.append(i)\n",
    "\n",
    "# Splitting ratios (default: 60% training, 20% validation, 20% test)\n",
    "# Note that the actual division may differ a bit\n",
    "val_ratio = 0.20\n",
    "test_ratio = 0.20\n",
    "\n",
    "for cls in classes_dir:\n",
    "    print('/n****** CLASS ', cls, '******')\n",
    "    print('> Creating class folders')\n",
    "    \n",
    "    os.makedirs(root_root_dir +'train/' + cls)\n",
    "    os.makedirs(root_root_dir +'val/' + cls)\n",
    "    os.makedirs(root_root_dir +'test/' + cls)\n",
    "    \n",
    "\n",
    "    src = root_dir + '/' + cls\n",
    "\n",
    "    allFileNames = os.listdir(src)\n",
    "    np.random.shuffle(allFileNames)\n",
    "    train_FileNames, val_FileNames, test_FileNames = np.split(np.array(allFileNames),\n",
    "                                                              [int(len(allFileNames)* (1 - (val_ratio + test_ratio))), \n",
    "                                                              int(len(allFileNames)* (1 - test_ratio))])\n",
    "\n",
    "\n",
    "    train_FileNames = [src+'/'+ name for name in train_FileNames.tolist()]\n",
    "    val_FileNames = [src+'/' + name for name in val_FileNames.tolist()]\n",
    "    test_FileNames = [src+'/' + name for name in test_FileNames.tolist()]\n",
    "\n",
    "    print('Total images in class ', cls, ': ', len(allFileNames))\n",
    "    print('Training: ', len(train_FileNames))\n",
    "    print('Validation: ', len(val_FileNames))\n",
    "    print('Testing: ', len(test_FileNames))\n",
    "\n",
    "    for name in train_FileNames:\n",
    "        shutil.copy(name, root_root_dir +'train/' + cls)\n",
    "\n",
    "    for name in val_FileNames:\n",
    "        shutil.copy(name, root_root_dir +'val/' + cls)\n",
    "\n",
    "    for name in test_FileNames:\n",
    "        shutil.copy(name, root_root_dir +'test/' + cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m6wPCMb4XAKQ"
   },
   "outputs": [],
   "source": [
    "# Zip the train-val-test sets again\n",
    "%cd /content/localdata\n",
    "!zip -r train.zip train/\n",
    "!zip -r val.zip val/\n",
    "!zip -r test.zip test/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZ8Y0bM7XSel"
   },
   "outputs": [],
   "source": [
    "# Copy the three zip files on Drive\n",
    "!cp train.zip '/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/dataset/train.zip'\n",
    "!cp val.zip '/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/dataset/val.zip'\n",
    "!cp test.zip '/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/dataset/test.zip'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ft0gJBJTfzHX"
   },
   "source": [
    "Here we create the zip of our second test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "urFRXU09ptpH"
   },
   "outputs": [],
   "source": [
    "!zip -r simili_dataset_NEW.zip 'Nuove_simili_al_dataset/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yEJx10zQaY5e"
   },
   "source": [
    "### Prepare data for training\n",
    "\n",
    "To speed up the training phase, the data set is loaded locally on the Colab virtual machine.\n",
    "So here the train, val and test zips are unzipped so to have all files available locally.\n",
    "[To do just once when the VM is started]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "executionInfo": {
     "elapsed": 2361,
     "status": "ok",
     "timestamp": 1600608258817,
     "user": {
      "displayName": "Christian Marinoni",
      "photoUrl": "",
      "userId": "06579677750120598109"
     },
     "user_tz": -120
    },
    "id": "iBlfTjdQaciv",
    "outputId": "1669cf6c-d73b-4588-a511-def0f1232a1d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/dataset\n"
     ]
    }
   ],
   "source": [
    "# Go to the folder where the zips are\n",
    "%cd '/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5wZ2n_MbdGj"
   },
   "outputs": [],
   "source": [
    "# Just a few seconds and all files will be on the content path of the Colab VM\n",
    "!unzip ./train.zip -d /content/localdata\n",
    "!unzip ./val.zip -d /content/localdata\n",
    "!unzip ./test.zip -d /content/localdata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d7PweZO2jtfZ"
   },
   "outputs": [],
   "source": [
    "# This does the same for the data set we built\n",
    "!unzip '/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/dataset/simili_dataset_NEW.zip' -d /content/localdata/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mgK8O9awYX-G"
   },
   "source": [
    "### Data Augmentation\n",
    "\n",
    "Data Augmentation is essential to improve results and reduce overfitting.\n",
    "\n",
    "Here we made experiments with different techniques:\n",
    "\n",
    "\n",
    "*   Horizontal Flip\n",
    "*   Vertical Flip\n",
    "*   Random crop\n",
    "*   Rotation range\n",
    "*   Brightness range\n",
    "\n",
    "In the final presentation we will show our considerations on the results we got during tests.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2056,
     "status": "ok",
     "timestamp": 1615383378821,
     "user": {
      "displayName": "lorenzo mandelli",
      "photoUrl": "",
      "userId": "01769209783218279689"
     },
     "user_tz": -60
    },
    "id": "hYh8DpiBxsIO",
    "outputId": "39d9144e-39d5-430e-8464-07c978f6a7fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.4.1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ozMeA_-xx22E"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os, datetime\n",
    "from tensorflow import keras\n",
    "import time\n",
    "from tensorflow.keras import models, layers, optimizers, applications\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ModelCheckpoint\n",
    "from skimage.transform import resize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Op3uJpCvyK6k"
   },
   "outputs": [],
   "source": [
    "# Go to the project folder on Drive\n",
    "%cd '/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AOWp8qLeyNYL"
   },
   "outputs": [],
   "source": [
    "# Set the path for all the \n",
    "train_data_dir = '/content/localdata/train/'\n",
    "val_data_dir = '/content/localdata/val/'\n",
    "test_data_dir = '/content/localdata/test/'\n",
    "target_image_size = (256,256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzWYIaedyO0G"
   },
   "outputs": [],
   "source": [
    "# Random crop is not natively implemented in the ImageDataGenerator class, so we\n",
    "# made our version\n",
    "def random_crop(image):\n",
    "    height, width = image.shape[:2]\n",
    "    random_array = np.random.random(size=3);\n",
    "    delta = int((width*0.5) * (1+random_array[0]*0.5))\n",
    "    x = int(random_array[1] * (width-delta))\n",
    "    y = int(random_array[2] * (height-delta))\n",
    "\n",
    "    image_crop = image[y:y+delta, x:x+delta, 0:3]\n",
    "    image_crop_resize = resize(image_crop, image.shape)\n",
    "    return image_crop_resize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a-Nx-rlZyPYl"
   },
   "outputs": [],
   "source": [
    "# Activate/Deactive the desidered data aug techniques\n",
    "image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255,\n",
    "                                                                  #horizontal_flip=True,\n",
    "                                                                  #brightness_range=[0.5,1.5],\n",
    "                                                                  #vertical_flip=True,\n",
    "                                                                  #rotation_range=10,\n",
    "                                                                  #preprocessing_function=random_crop\n",
    "                                                                  )\n",
    "# No data augmentation is applied to the test set\n",
    "test_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KU5m2PD-ySwU"
   },
   "outputs": [],
   "source": [
    "train_data_gen = image_generator.flow_from_directory(\n",
    "                                                     directory=train_data_dir,\n",
    "                                                     color_mode='rgb',\n",
    "                                                     batch_size=32,\n",
    "                                                     shuffle=True,\n",
    "                                                     seed=123,\n",
    "                                                     target_size=target_image_size\n",
    "                                                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WrTA6RXxyUnL"
   },
   "outputs": [],
   "source": [
    "val_data_gen = image_generator.flow_from_directory(\n",
    "                                                    directory=val_data_dir,\n",
    "                                                    batch_size=32,\n",
    "                                                    color_mode='rgb',\n",
    "                                                    shuffle=True,\n",
    "                                                    seed=123,\n",
    "                                                    target_size=target_image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MAKqPhsUaCxB"
   },
   "outputs": [],
   "source": [
    "test_data_gen = test_image_generator.flow_from_directory(\n",
    "                                                    directory=test_data_dir,\n",
    "                                                    batch_size=32,\n",
    "                                                    shuffle=False,\n",
    "                                                    target_size=target_image_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JgIditOiFOTp"
   },
   "outputs": [],
   "source": [
    "# We also create two simple dict with class name <-> index class\n",
    "classes_dict = train_data_gen.class_indices\n",
    "to_classes_dict = {index: classe for classe, index in classes_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ws2uV7Zsge-V"
   },
   "outputs": [],
   "source": [
    "# Here there's a different dict where each class is associated to the image count\n",
    "# of that class (used to print the following histogram)\n",
    "classes = {}\n",
    "for i in os.listdir('./color/'):\n",
    "  path = './color/' + i + '/'\n",
    "  count = 0\n",
    "  for f in os.listdir(path):\n",
    "    if os.path.isfile(os.path.join(path, f)):\n",
    "        count += 1\n",
    "  classes[i] = count\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5WTkHFXZjauJ"
   },
   "outputs": [],
   "source": [
    "# Generate the histogram of the class distribution\n",
    "plt.figure(figsize=(15,10))\n",
    "x = [elem for elem in classes.keys()]\n",
    "y = [n for n in classes.values()]\n",
    "plt.gcf().subplots_adjust(bottom=0.42)\n",
    "plt.bar(x, y, color='b')\n",
    "plt.xticks(x, classes.keys(), rotation='vertical')\n",
    "plt.savefig('/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/dataset/images_distribution.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "txOq24RjY5-9"
   },
   "source": [
    "### Model definition: our base model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z1RgVdeaiL13"
   },
   "outputs": [],
   "source": [
    "# To set the learning rate\n",
    "optimizer = keras.optimizers.Adam(lr=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F6s4g-kL1dX7"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(layers.Conv2D(32, (3,3), padding='same', activation='relu', input_shape=(256, 256, 3)))\n",
    "  model.add(layers.MaxPooling2D())\n",
    "  model.add(layers.BatchNormalization())\n",
    "\n",
    "  model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "  model.add(layers.MaxPooling2D())\n",
    "  model.add(layers.BatchNormalization())\n",
    "\n",
    "  model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "  model.add(layers.MaxPooling2D())\n",
    "  model.add(layers.BatchNormalization())\n",
    "\n",
    "  model.add(layers.Conv2D(64, (3,3), padding='same', activation='relu'))\n",
    "  model.add(layers.MaxPooling2D())\n",
    "  model.add(layers.BatchNormalization())\n",
    "\n",
    "  model.add(layers.Conv2D(128, (3,3), padding='same', activation='relu'))\n",
    "  model.add(layers.MaxPooling2D())\n",
    "  model.add(layers.BatchNormalization())\n",
    "  \n",
    "  model.add(layers.Flatten())\n",
    "\n",
    "\n",
    "  model.add(layers.Dense(128, activation='relu'))\n",
    "  model.add(layers.Dropout(0.5))\n",
    "  model.add(layers.Dense(128, activation='relu'))\n",
    "  model.add(layers.Dropout(0.5))\n",
    "  model.add(layers.Dense(38, activation='softmax'))\n",
    "  \n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                #optimizer = optimizer,\n",
    "                metrics=['accuracy'])\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 925,
     "status": "ok",
     "timestamp": 1615383407800,
     "user": {
      "displayName": "lorenzo mandelli",
      "photoUrl": "",
      "userId": "01769209783218279689"
     },
     "user_tz": -60
    },
    "id": "y0k0svDE2O5D",
    "outputId": "fd4cd288-e470-41dc-d8bd-dfd3114ea14c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 256, 256, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 128, 128, 32)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 128, 128, 32)      128       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 128, 128, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 64, 64, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 64, 64, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 64, 64, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 32, 32, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 16, 16, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 8, 8, 128)         512       \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 8192)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               1048704   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 128)               16512     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 38)                4902      \n",
      "=================================================================\n",
      "Total params: 1,238,630\n",
      "Trainable params: 1,237,926\n",
      "Non-trainable params: 704\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7DZFV0DAIKJ5"
   },
   "source": [
    "### Model definition: Pretrained model\n",
    "\n",
    "We made tests with VGG, ResNet, Inception and DenseNet implementing both feature extraction and fine tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sdtxtXi4yVdD"
   },
   "outputs": [],
   "source": [
    "resnet = tf.keras.applications.ResNet50V2(weights='imagenet', include_top=False, input_shape=(256, 256, 3))\n",
    "resnet.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xMA__0Kn1POI"
   },
   "outputs": [],
   "source": [
    "# To freeze a subset of layers: resnet.layers[:-X]\n",
    "for layer in resnet.layers:\n",
    "  layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ms6CHrwQI0Ps"
   },
   "outputs": [],
   "source": [
    "def build_model():\n",
    "  model = models.Sequential()\n",
    "  model.add(resnet)\n",
    "  model.add(layers.GlobalAveragePooling2D())\n",
    "  model.add(layers.Dense(1024, activation='relu'))\n",
    "  model.add(layers.Dense(38, activation='softmax'))\n",
    "\n",
    "  model.compile(loss='categorical_crossentropy',\n",
    "                optimizer='adam',\n",
    "                metrics=['accuracy'])\n",
    "  \n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y-pFE3ddI0Pw"
   },
   "outputs": [],
   "source": [
    "model = build_model()\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T8HRqyQdH1Hc"
   },
   "source": [
    "### Tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "executionInfo": {
     "elapsed": 1037,
     "status": "ok",
     "timestamp": 1600519415162,
     "user": {
      "displayName": "Christian Marinoni",
      "photoUrl": "",
      "userId": "06579677750120598109"
     },
     "user_tz": -120
    },
    "id": "6irVzyJl4uiZ",
    "outputId": "8b2cbf6d-57b2-454a-8627-a9f96d09e5f9"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/logs/20200919-124334'"
      ]
     },
     "execution_count": 30,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overwrite_log_dir = \"\"\n",
    "\n",
    "root = os.getcwd()\n",
    "if (overwrite_log_dir==\"\"):\n",
    "  log_dir = os.path.join(root, \"logs\", datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "else: \n",
    "  log_dir = overwrite_log_dir\n",
    "log_dir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UUy3doBK5rPZ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mYsC46BC5JuJ"
   },
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aTL8sOgY8Szs"
   },
   "outputs": [],
   "source": [
    "# ONLY IF NEEDED!\n",
    "!rm -rf '/content/drive/My Drive/Università/Vision and Perception/Project/logs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wN-JOcfHySm0"
   },
   "outputs": [],
   "source": [
    "# It should be used (changing the pid) to kill the Tensorflow process when it\n",
    "# starts to throw a tantrum. Though, rarely killed the process\n",
    "!kill 11324"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5hIAmzv5Nlt"
   },
   "outputs": [],
   "source": [
    "%tensorboard --logdir logs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LHIys0WRez_8"
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQADlJIm48xT"
   },
   "outputs": [],
   "source": [
    "# FE = Feature Extraction\n",
    "# FT = Fine Tuning\n",
    "# HF = Horizontal flip\n",
    "# RR = Rotation range\n",
    "# RC = Random cropping\n",
    "# RB = Random brightness\n",
    "# GAP = Global Average Pooling\n",
    "# FL = Flatten\n",
    "# Dx = Dense layer with x units (i.e. D512)\n",
    "\n",
    "# Name template: test{X}_{nameModel}_{typeTraining}_{addedLayers}_{dataAugTechniques}.h5\n",
    "\n",
    "path_model = '/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/models/test123_ourmodel_RB_RR_HF.h5'\n",
    "\n",
    "callbacks = [\n",
    "    TensorBoard(log_dir, update_freq = 50),\n",
    "    EarlyStopping(monitor='val_accuracy', patience=5),\n",
    "    ModelCheckpoint(path_model, save_best_only=True)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7sAAwN0CXCKp"
   },
   "outputs": [],
   "source": [
    "# Set class weights used for weighting the loss function, thus keeping into account\n",
    "# the imbalance of the data set.\n",
    "# Here the output is a dict passed when training the model\n",
    "class_weights = class_weight.compute_class_weight('balanced',\n",
    "                                                 list(to_classes_dict.keys()),\n",
    "                                                 train_data_gen.classes.tolist())\n",
    "dict_weights ={}\n",
    "for i in range(len(class_weights)):\n",
    "  dict_weights[i] = class_weights[i]\n",
    "print(dict_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cCjlWa0Y5SxF"
   },
   "outputs": [],
   "source": [
    "history = model.fit(train_data_gen, epochs=40,\n",
    "                    validation_data=val_data_gen,\n",
    "                    initial_epoch=0,\n",
    "                    shuffle=True,\n",
    "                    max_queue_size = 256,\n",
    "                    callbacks=callbacks,\n",
    "                    #class_weight=class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uDss4sTMvcKp"
   },
   "outputs": [],
   "source": [
    "# accuracy plot\n",
    "plt.plot(history.history['accuracy'])\n",
    "plt.plot(history.history['val_accuracy'])\n",
    "plt.title('model accuracy')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('test_123_plot_accuracy')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# loss plot\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "plt.savefig('test_123_plot_loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ws06nCb9IRc0"
   },
   "source": [
    "### Test phase\n",
    "\n",
    "In this part of the notebook we made several tests on both the test set of the main data set and on an hand-made test set*.\n",
    "\n",
    "*We visited some friends, relatives and local farmers and collected more than 350 images of leaves. Of these photos we selected ~200 images to form our test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfjoGpjeEZcc"
   },
   "source": [
    "#### Only for the hand-made test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_ReKIFYaFzKs"
   },
   "outputs": [],
   "source": [
    "folder_test_set = 'Nuove_simili_al_dataset'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Anp7pXPvFzKu"
   },
   "outputs": [],
   "source": [
    "for folder in classes_dict.keys():\n",
    "  if (not os.path.isdir( '/content/localdata/' + folder_test_set +'/' + folder)):\n",
    "    os.makedirs('/content/localdata/' + folder_test_set +'/' + folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b_zuO3wMFzKw"
   },
   "outputs": [],
   "source": [
    "test_image_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255)\n",
    "test_data_gen = test_image_generator.flow_from_directory(\n",
    "                                                    directory='/content/localdata/' + folder_test_set +'/',\n",
    "                                                    batch_size=5,\n",
    "                                                    target_size=(256,256),\n",
    "                                                    shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o4chnM0PF55L"
   },
   "source": [
    "#### Evaluate, Confusion Matrix and Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yOg5KGeFEkTu"
   },
   "outputs": [],
   "source": [
    "model.evaluate(test_data_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SRlLWLm7GKD0"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import seaborn as sn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgC8COFNGKD5"
   },
   "outputs": [],
   "source": [
    "y_pred_all = model.predict(test_data_gen)\n",
    "y_pred = np.argmax(y_pred_all, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7R2CcV_aGTS5"
   },
   "outputs": [],
   "source": [
    "print('Confusion Matrix')\n",
    "conf_matrix=confusion_matrix(test_data_gen.classes, y_pred)\n",
    "conf_matrix = tf.math.confusion_matrix(test_data_gen.classes, y_pred)\n",
    "plt.figure(figsize = (20,12))\n",
    "sn.heatmap(conf_matrix, annot=True, fmt='.0f', cmap='coolwarm', xticklabels=classes_dict.keys(), yticklabels=classes_dict.keys())\n",
    "print('Classification Report')\n",
    "print(classification_report(test_data_gen.classes, y_pred, zero_division=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yqn2rmFlQHiR"
   },
   "outputs": [],
   "source": [
    "# This module here is used to get a colorful version of the classification report.\n",
    "# It is not really essential, but it helps in identifying critical values instantly\n",
    "# The code has been taken from Stack Overflow. We left all the credits there.\n",
    "\n",
    "def show_values(pc, fmt=\"%.2f\", **kw):\n",
    "    '''\n",
    "    Heatmap with text in each cell with matplotlib's pyplot\n",
    "    Source: https://stackoverflow.com/a/25074150/395857 \n",
    "    By HYRY\n",
    "    '''\n",
    "    import itertools\n",
    "    pc.update_scalarmappable()\n",
    "    ax = pc.axes\n",
    "    for p, color, value in zip(pc.get_paths(), pc.get_facecolors(), pc.get_array()):\n",
    "        x, y = p.vertices[:-2, :].mean(0)\n",
    "        if np.all(color[:3] > 0.5):\n",
    "            color = (0.0, 0.0, 0.0)\n",
    "        else:\n",
    "            color = (1.0, 1.0, 1.0)\n",
    "        ax.text(x, y, fmt % value, ha=\"center\", va=\"center\", color=color, **kw)\n",
    "\n",
    "\n",
    "def cm2inch(*tupl):\n",
    "    '''\n",
    "    Specify figure size in centimeter in matplotlib\n",
    "    Source: https://stackoverflow.com/a/22787457/395857\n",
    "    By gns-ank\n",
    "    '''\n",
    "    inch = 2.54\n",
    "    if type(tupl[0]) == tuple:\n",
    "        return tuple(i/inch for i in tupl[0])\n",
    "    else:\n",
    "        return tuple(i/inch for i in tupl)\n",
    "\n",
    "\n",
    "def heatmap(AUC, title, xlabel, ylabel, xticklabels, yticklabels, figure_width=40, figure_height=20, correct_orientation=False, cmap='RdBu'):\n",
    "    '''\n",
    "    Inspired by:\n",
    "    - https://stackoverflow.com/a/16124677/395857 \n",
    "    - https://stackoverflow.com/a/25074150/395857\n",
    "    '''\n",
    "\n",
    "    # Plot it out\n",
    "    fig, ax = plt.subplots()    \n",
    "    #c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap='RdBu', vmin=0.0, vmax=1.0)\n",
    "    c = ax.pcolor(AUC, edgecolors='k', linestyle= 'dashed', linewidths=0.2, cmap=cmap)\n",
    "\n",
    "    # put the major ticks at the middle of each cell\n",
    "    ax.set_yticks(np.arange(AUC.shape[0]) + 0.5, minor=False)\n",
    "    ax.set_xticks(np.arange(AUC.shape[1]) + 0.5, minor=False)\n",
    "\n",
    "    # set tick labels\n",
    "    #ax.set_xticklabels(np.arange(1,AUC.shape[1]+1), minor=False)\n",
    "    ax.set_xticklabels(xticklabels, minor=False)\n",
    "    ax.set_yticklabels(yticklabels, minor=False)\n",
    "\n",
    "    # set title and x/y labels\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)      \n",
    "\n",
    "    # Remove last blank column\n",
    "    plt.xlim( (0, AUC.shape[1]) )\n",
    "\n",
    "    # Turn off all the ticks\n",
    "    ax = plt.gca()    \n",
    "    for t in ax.xaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "    for t in ax.yaxis.get_major_ticks():\n",
    "        t.tick1On = False\n",
    "        t.tick2On = False\n",
    "\n",
    "    # Add color bar\n",
    "    plt.colorbar(c)\n",
    "\n",
    "    # Add text in each cell \n",
    "    show_values(c)\n",
    "\n",
    "    # Proper orientation (origin at the top left instead of bottom left)\n",
    "    if correct_orientation:\n",
    "        ax.invert_yaxis()\n",
    "        ax.xaxis.tick_top()       \n",
    "\n",
    "    # resize \n",
    "    fig = plt.gcf()\n",
    "    #fig.set_size_inches(cm2inch(40, 20))\n",
    "    #fig.set_size_inches(cm2inch(40*4, 20*4))\n",
    "    fig.set_size_inches(cm2inch(figure_width, figure_height))\n",
    "\n",
    "\n",
    "\n",
    "def plot_classification_report(classification_report, title='Classification report ', cmap='RdBu'):\n",
    "    '''\n",
    "    Plot scikit-learn classification report.\n",
    "    Extension based on https://stackoverflow.com/a/31689645/395857 \n",
    "    '''\n",
    "    lines = classification_report.split('\\n')\n",
    "\n",
    "    classes = []\n",
    "    plotMat = []\n",
    "    support = []\n",
    "    class_names = []\n",
    "    for line in lines[2 : (len(lines) - 4)]:\n",
    "        t = line.strip().split()\n",
    "        if len(t) < 2: continue\n",
    "        classes.append(t[0])\n",
    "        v = [float(x) for x in t[1: len(t) - 1]]\n",
    "        support.append(int(t[-1]))\n",
    "        class_names.append(to_classes_dict[int(t[0])])\n",
    "        print(v)\n",
    "        plotMat.append(v)\n",
    "\n",
    "    print('plotMat: {0}'.format(plotMat))\n",
    "    print('support: {0}'.format(support))\n",
    "\n",
    "    xlabel = 'Metrics'\n",
    "    ylabel = 'Classes'\n",
    "    xticklabels = ['Precision', 'Recall', 'F1-score']\n",
    "    yticklabels = ['{0} ({1})'.format(class_names[idx], sup) for idx, sup  in enumerate(support)]\n",
    "    figure_width = 25\n",
    "    figure_height = len(class_names) + 7\n",
    "    correct_orientation = False\n",
    "    heatmap(np.array(plotMat), title, xlabel, ylabel, xticklabels, yticklabels, figure_width, figure_height, correct_orientation, cmap=cmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4O7BVjttQLSy"
   },
   "outputs": [],
   "source": [
    "plot_classification_report(classification_report(test_data_gen.classes, y_pred), \"Classification Report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PPsz1Y-NHkvL"
   },
   "source": [
    "Some small tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CbX78MOSF4J0"
   },
   "outputs": [],
   "source": [
    "# Pick one image and make the predict on it\n",
    "image = keras.preprocessing.image.load_img('/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/dataset2/squash_test.jpg')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8OeBsoYKJIOw"
   },
   "outputs": [],
   "source": [
    "image = resize(np.array(image), (256,256))\n",
    "image = image2.reshape((1,256,256,3))\n",
    "image.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "no8FSk40KJEt"
   },
   "outputs": [],
   "source": [
    "predictions = model.predict(image, batch_size = 1)\n",
    "to_classes_dict[np.argmax(predictions)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BWYTLSwjHxeU"
   },
   "source": [
    "Load or save a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zByhgazQ-HMs"
   },
   "outputs": [],
   "source": [
    "# Save a model\n",
    "keras.models.save_model(model,'/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/models/nostro_test88_fine.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aZYbR_-tLQGW"
   },
   "outputs": [],
   "source": [
    "# Load a previously trained model\n",
    "model = keras.models.load_model('/content/drive/My Drive/Università/Magistrale/Vision and Perception/Project/models/test2_DenseNet201_FT_GAP_D1024_HF_RR_RC.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QP1mgZ2TTBPg"
   },
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "txOq24RjY5-9",
    "7DZFV0DAIKJ5",
    "T8HRqyQdH1Hc",
    "LHIys0WRez_8",
    "Ws06nCb9IRc0",
    "lfjoGpjeEZcc",
    "o4chnM0PF55L"
   ],
   "name": "Vision and Perception - Plant Disease.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
